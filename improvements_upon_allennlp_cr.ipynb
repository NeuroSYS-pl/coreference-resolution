{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('anaphors': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d03d5abba74e86978e68a6ade085aee69b84e82c031d22c409c19824e6ca40c1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from spacy.tokens import Doc, Span\n",
    "from utils import load_models, print_clusters, print_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "predictor, nlp = load_models()"
   ]
  },
  {
   "source": [
    "## The problem\n",
    "AllenNLP coreference resolution models seems to find better clusters - numerous clusters that are usually more accurate than the ones found by Huggingface NeuralCoref model. However, the biggest problem lies in the next step - the step of replacing found mentions with the most meaningfull spans from each clusters (that we call the \"heads\"). We've found a couple of easy-to-fix problems which seem to lead to errors most often. Our ideas can be summed up as:\n",
    "- not resolving coreferences in the clusters that doesn't contain any noun phrases (usually it comes down to the clusters composed only of pronouns),\n",
    "- chosing the head of the cluster which is a noun phrase (isn't a pronoun),\n",
    "- resolving only the inner span in the case of nested coreferent mentions.\n",
    "\n",
    "We show all of our improvements by example - for more details please refer to our [third and last blog post]() that contain more details."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Original AllenNLP impelemntation of the `replace_corefs` method\n",
    "\n",
    "We extract the main \"logic\" into the separate function that will be used in our every method as we leave the core of AllenNLP's logic untouched. So as for now we will compare our solutions to the `original_replace_corefs` method implemented in AllenNLP `coref.py`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_logic_part(document: Doc, coref: List[int], resolved: List[str], mention_span: Span):\n",
    "    final_token = document[coref[1]]\n",
    "    if final_token.tag_ in [\"PRP$\", \"POS\"]:\n",
    "        resolved[coref[0]] = mention_span.text + \"'s\" + final_token.whitespace_\n",
    "    else:\n",
    "        resolved[coref[0]] = mention_span.text + final_token.whitespace_\n",
    "    for i in range(coref[0] + 1, coref[1] + 1):\n",
    "        resolved[i] = \"\"\n",
    "    return resolved\n",
    "\n",
    "\n",
    "def original_replace_corefs(document: Doc, clusters: List[List[List[int]]]) -> str:\n",
    "    resolved = list(tok.text_with_ws for tok in document)\n",
    "\n",
    "    for cluster in clusters:\n",
    "        mention_start, mention_end = cluster[0][0], cluster[0][1] + 1\n",
    "        mention_span = document[mention_start:mention_end]\n",
    "\n",
    "        for coref in cluster[1:]:\n",
    "            core_logic_part(document, coref, resolved, mention_span)\n",
    "\n",
    "    return \"\".join(resolved)"
   ]
  },
  {
   "source": [
    "## Improvements\n",
    "### Redundant clusters - lack of a meaningfull mention that could become the head\n",
    "We completely ignore (we don't resove them at all) the clusters that doesn't contain any noun phrase."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_noun_indices(doc: Doc, cluster: List[List[int]]) -> List[int]:\n",
    "    spans = [doc[span[0]:span[1]+1] for span in cluster]\n",
    "    spans_pos = [[token.pos_ for token in span] for span in spans]\n",
    "    span_noun_indices = [i for i, span_pos in enumerate(spans_pos)\n",
    "        if any(pos in span_pos for pos in ['NOUN', 'PROPN'])]\n",
    "    return span_noun_indices\n",
    "\n",
    "def improved_replace_corefs(document, clusters):\n",
    "    resolved = list(tok.text_with_ws for tok in document)\n",
    "\n",
    "    for cluster in clusters:\n",
    "        noun_indices = get_span_noun_indices(document, cluster)\n",
    "\n",
    "        if noun_indices:\n",
    "            mention_start, mention_end = cluster[0][0], cluster[0][1] + 1\n",
    "            mention_span = document[mention_start:mention_end]\n",
    "\n",
    "            for coref in cluster[1:]:\n",
    "                core_logic_part(document, coref, resolved, mention_span)\n",
    "\n",
    "    return \"\".join(resolved)"
   ]
  },
  {
   "source": [
    "**Example**  \n",
    "We want to take our code and create a game. Let's remind ourselves how to do that.  \n",
    "- We --> We; our; 's; ourselves  \n",
    "- create --> create; that"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"We want to take our code and create a game. Let's remind ourselves how to do that.\"\n",
    "_, _, _, document, clusters = predictor.predict(text).values()\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AllenNLP original replace_corefs:\nWe want to take We's code and create a game. LetWe remind We how to do create.\n\nOur improved replace_corefs:\nWe want to take our code and create a game. Let's remind ourselves how to do that.\n"
     ]
    }
   ],
   "source": [
    "print_comparison(original_replace_corefs(doc, clusters), improved_replace_corefs(doc, clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}